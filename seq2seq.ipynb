{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "seq2seq",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsX0L1sG1iZj"
      },
      "source": [
        "## Neural translation model - English to German Translation using seq2seq model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VyTvxPN1iZn"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import unicodedata\n",
        "import re\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi9Dq6vv3FVO"
      },
      "source": [
        "##Data import\n",
        "\n",
        "Dataset downlad link:\n",
        "\n",
        "https://drive.google.com/open?id=1KczOciG7sYY7SB9UlBeRP1T9659b121Q\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8PetPpw1iZu"
      },
      "source": [
        "# load the dataset\n",
        "NUM_EXAMPLES = 20000\n",
        "data_examples = []\n",
        "with open('./deu.txt', 'r', encoding='utf8') as f:\n",
        "    for line in f.readlines():\n",
        "        if len(data_examples) < NUM_EXAMPLES:\n",
        "            data_examples.append(line)\n",
        "        else:\n",
        "            break"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JumLjJ631iZy"
      },
      "source": [
        "# Preprocess English and German sentences\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r\"ü\", 'ue', sentence)\n",
        "    sentence = re.sub(r\"ä\", 'ae', sentence)\n",
        "    sentence = re.sub(r\"ö\", 'oe', sentence)\n",
        "    sentence = re.sub(r'ß', 'ss', sentence)\n",
        "    \n",
        "    sentence = unicode_to_ascii(sentence)\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r\"[^a-z?.!,']+\", \" \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    \n",
        "    return sentence.strip()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z70nu6_01iZ3"
      },
      "source": [
        "\n",
        "## Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9G20C4bk1iZ4"
      },
      "source": [
        "# separates list of english and german sentences and preprosess them\n",
        "def get_sentence(data_examples):\n",
        "    english_sentence = []\n",
        "    german_sentence = []\n",
        "    for line in data_examples:\n",
        "        temp_list = line.strip().split('\\t')\n",
        "        english_sentence.append(preprocess_sentence(temp_list[0]))\n",
        "        german_sentence.append(preprocess_sentence(temp_list[1]))\n",
        "    return english_sentence, german_sentence\n",
        "\n",
        "english_sentences, german_sentences = get_sentence(data_examples)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEdfKMhEs-MR"
      },
      "source": [
        "# Add a special \"<start>\" and \"<end>\" token to the beginning and end of every German sentence.\n",
        "def add_start_and_end_token(german_sentences):\n",
        "    start_token = '<start> '\n",
        "    end_token = ' <end>'\n",
        "    return [start_token + line + end_token for line in german_sentences]\n",
        "\n",
        "german_sentences_with_token = add_start_and_end_token(german_sentences)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAIUW8Og0dRE"
      },
      "source": [
        "# tokenize the German sentences\n",
        "def get_tokenizer(sentences):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    return tokenizer\n",
        "german_tokenizer = get_tokenizer(german_sentences_with_token)\n",
        "german_tokenizer_max_index = max(german_tokenizer.index_word.keys())\n",
        "german_seq = german_tokenizer.texts_to_sequences(german_sentences_with_token)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO_IT1j96S7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "499ab68f-be91-4245-c415-f1d79db205fa"
      },
      "source": [
        "# 5 randomly chosen examples of (preprocessed) English and German sentence pairs.\n",
        "idx = tf.random.uniform((5,), minval=0, maxval=len(german_seq), dtype=tf.int32).numpy()\n",
        "for i, id in enumerate(idx):\n",
        "    if i != 0:\n",
        "        print('\\n==================================================================================\\n')\n",
        "\n",
        "    print(f'English translation: {english_sentences[id]}')\n",
        "    print(f'German translation: {german_sentences_with_token[id]}')\n",
        "    print(f'German tokenized sequence: {german_seq[id]}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English translation: throw the dice .\n",
            "German translation: <start> wuerfele ! <end>\n",
            "German tokenized sequence: [1, 2598, 9, 2]\n",
            "\n",
            "==================================================================================\n",
            "\n",
            "English translation: beat it .\n",
            "German translation: <start> geh weg ! <end>\n",
            "German tokenized sequence: [1, 89, 102, 9, 2]\n",
            "\n",
            "==================================================================================\n",
            "\n",
            "English translation: i want to win .\n",
            "German translation: <start> ich will gewinnen . <end>\n",
            "German tokenized sequence: [1, 4, 66, 227, 3, 2]\n",
            "\n",
            "==================================================================================\n",
            "\n",
            "English translation: i understand tom .\n",
            "German translation: <start> ich verstehe tom . <end>\n",
            "German tokenized sequence: [1, 4, 336, 5, 3, 2]\n",
            "\n",
            "==================================================================================\n",
            "\n",
            "English translation: what a funny man !\n",
            "German translation: <start> was fuer ein lustiger mann ! <end>\n",
            "German tokenized sequence: [1, 38, 77, 19, 5374, 193, 9, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHbziF9K9ISq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f11f32f9-dc6d-486c-a3dd-c625431a675d"
      },
      "source": [
        "# Pads the end of the tokenized German sequences with zeros, and batch the complete set of sequences into one numpy array.\n",
        "german_padded_seq = tf.keras.preprocessing.sequence.pad_sequences(german_seq, padding='post')\n",
        "print(f'german_padded_seq.shape = {german_padded_seq.shape}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "german_padded_seq.shape = (20000, 14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foL7Ihs21iaP"
      },
      "source": [
        "##Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywZgobCh1iaR"
      },
      "source": [
        "# Load embedding module from Tensorflow Hub\n",
        "\n",
        "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\", \n",
        "                                 output_shape=[128], input_shape=[], dtype=tf.string)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiY8QEDp1iaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a49ec1-d877-4804-ef4a-ad526f62f3e3"
      },
      "source": [
        "# Test the layer\n",
        "embedding_layer(tf.constant([\"these\", \"aren't\", \"the\", \"droids\", \"you're\", \"looking\", \"for\"])).shape[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-BUJOl_1iaZ"
      },
      "source": [
        "# Creates a random training and validation set split of the data, reserving 20% of the data for validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "split = train_test_split(english_sentences, german_padded_seq, test_size=0.2, random_state=5)\n",
        "english_sentences_train, english_sentences_test = split[:2]\n",
        "german_padded_seq_train, german_padded_seq_test = split[2:]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w6rM8Bl1iad"
      },
      "source": [
        "# Loads the training and validation sets into a tf.data.Dataset object, passing in a tuple of English and German data for both training and validation sets.\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices((english_sentences_train, german_padded_seq_train))\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((english_sentences_test, german_padded_seq_test))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGqlxuqZ1ias"
      },
      "source": [
        "# function to map over the datasets that splits each English sentence at spaces.\n",
        "def split_english_sentences(dataset):\n",
        "    def func(english_sentence, german_padded_seq):\n",
        "        return tf.strings.split(english_sentence), german_padded_seq\n",
        "    dataset = dataset.map(func)\n",
        "    return dataset\n",
        "    \n",
        "training_dataset = split_english_sentences(training_dataset)\n",
        "validation_dataset = split_english_sentences(validation_dataset)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDPkbOx7rR3o"
      },
      "source": [
        "# function to map over the datasets that embeds each sequence of English words using the loaded embedding layer/model.\n",
        "def embed_english_words(dataset, embedding_layer):\n",
        "    dataset = dataset.map(lambda x,y: (embedding_layer(x), y))\n",
        "    return dataset\n",
        "    \n",
        "training_dataset = embed_english_words(training_dataset, embedding_layer)\n",
        "validation_dataset = embed_english_words(validation_dataset, embedding_layer)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdqSXEoX1iav"
      },
      "source": [
        "# function to filter out dataset examples where the English sentence is more than 13 (embedded) tokens in length. \n",
        "def filter_english_seq(dataset):\n",
        "    dataset = dataset.filter(lambda x,y: len(x) <= 13)\n",
        "    return dataset\n",
        "  \n",
        "training_dataset = filter_english_seq(training_dataset)\n",
        "validation_dataset = filter_english_seq(validation_dataset)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpQdGn9sBw2-"
      },
      "source": [
        "# function to map over the datasets that pads each English sequence of embeddings with \n",
        "# some distinct padding value before the sequence, so that each sequence is length 13.\n",
        "def pad_english_seq(dataset):\n",
        "    def func(x, y):\n",
        "        x = tf.pad(x, paddings=[[13 - tf.shape(x)[0], 0],[0,0]], constant_values=0)\n",
        "        return x,y\n",
        "    dataset = dataset.map(func)\n",
        "    return dataset\n",
        "\n",
        "training_dataset = pad_english_seq(training_dataset)\n",
        "validation_dataset = pad_english_seq(validation_dataset)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KTOK1F2o5ye"
      },
      "source": [
        "# Batches both training and validation Datasets with a batch size of 16\n",
        "def get_batched_dataset(dataset, training=False, batch_size=16):\n",
        "    if training:\n",
        "      dataset = dataset.shuffle(1000, reshuffle_each_iteration=True)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "training_batched_dataset = get_batched_dataset(training_dataset, training=True)\n",
        "validation_batched_dataset = get_batched_dataset(validation_dataset)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHB7YcXN6Zil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c3485ea-5293-472f-f2d6-310d1499509c"
      },
      "source": [
        "# element_spec property for the training and validation Dataset\n",
        "print(f'training_batched_dataset.element_spec = {training_batched_dataset.element_spec}')\n",
        "print(f'validation_batched_dataset.element_spec = {validation_batched_dataset.element_spec}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training_batched_dataset.element_spec = (TensorSpec(shape=(None, None, 128), dtype=tf.float32, name=None), TensorSpec(shape=(None, 14), dtype=tf.int32, name=None))\n",
            "validation_batched_dataset.element_spec = (TensorSpec(shape=(None, None, 128), dtype=tf.float32, name=None), TensorSpec(shape=(None, 14), dtype=tf.int32, name=None))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re_kZCgV7NLR"
      },
      "source": [
        "english_batch, german_batch = next(iter(training_batched_dataset.take(1)))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC1sZV6p7ALj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6dcd11-a3bd-4afe-de45-89eff4d0a9f5"
      },
      "source": [
        "# shape of the English data example from the training Dataset.\n",
        "print(f'shape of the English data example = {english_batch.shape}')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of the English data example = (16, 13, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsCKnnku7xID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6a897c1-d6aa-4e71-a88f-c80ce6773bcb"
      },
      "source": [
        "# German data example Tensor from the validation Dataset.\n",
        "print(f'shape of the German data example = \\n{german_batch}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of the German data example = \n",
            "[[   1    5    6   19 1837    3    2    0    0    0    0    0    0    0]\n",
            " [   1 1306  477   25    5    3    2    0    0    0    0    0    0    0]\n",
            " [   1    4   15  660    3    2    0    0    0    0    0    0    0    0]\n",
            " [   1    4   15 1361    3    2    0    0    0    0    0    0    0    0]\n",
            " [   1   11    6   19 4012  107    3    2    0    0    0    0    0    0]\n",
            " [   1    5   51  951    3    2    0    0    0    0    0    0    0    0]\n",
            " [   1    4  336   11   12    3    2    0    0    0    0    0    0    0]\n",
            " [   1    4   51   11  112    3    2    0    0    0    0    0    0    0]\n",
            " [   1  140   27   10  487    7    2    0    0    0    0    0    0    0]\n",
            " [   1    4 1043   53 1705    3    2    0    0    0    0    0    0    0]\n",
            " [   1    8  114  249   41    3    2    0    0    0    0    0    0    0]\n",
            " [   1   14    6 2863    3    2    0    0    0    0    0    0    0    0]\n",
            " [   1    4   30   33   62   12  102    3    2    0    0    0    0    0]\n",
            " [   1  746 3071   71 1409    3    2    0    0    0    0    0    0    0]\n",
            " [   1   13   58 2436    3    2    0    0    0    0    0    0    0    0]\n",
            " [   1    5    6 1149    3    2    0    0    0    0    0    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isIYhjq01iay"
      },
      "source": [
        "## Custom layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg9hjZz11ia0"
      },
      "source": [
        "# layer that takes a batch of English data examples from one of the \n",
        "# Datasets, and adds a learned embedded ‘end’ token to the end of each sequence. \n",
        "class EndTokenEmbeddingLayer(tf.keras.layers.Layer):\n",
        "  \n",
        "    def __init__(self, **kwargs):\n",
        "        super(EndTokenEmbeddingLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        initializer = tf.random_normal_initializer()\n",
        "        self.embedding = tf.Variable(initial_value=initializer(shape=(input_shape[-1],)))\n",
        "\n",
        "    def call(self, input):\n",
        "        x = tf.map_fn(lambda x: tf.concat([x, tf.reshape(tf.convert_to_tensor(self.embedding), (1,-1))], axis=0), input)\n",
        "        return x"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7oUT8278kIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb9a0b0c-6272-4712-fff6-cefd4ae895a9"
      },
      "source": [
        "english_batch, german_batch = next(iter(training_batched_dataset.take(1)))\n",
        "print(f'shape after passing through EndTokenEmbeddingLayer = {EndTokenEmbeddingLayer()(english_batch).shape}')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape after passing through EndTokenEmbeddingLayer = (16, 14, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAd3i4_y1ia-"
      },
      "source": [
        "## Encoder network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R2LqbfV1ia_"
      },
      "source": [
        "def get_encoder(input_shape):\n",
        "\n",
        "    model_input = tf.keras.Input(shape=input_shape, name='input')\n",
        "    x = EndTokenEmbeddingLayer(name='end_token_embedding_layer')(model_input)\n",
        "    x = tf.keras.layers.Masking(name='masking_layer')(x)\n",
        "    output, hidden_state, cell_state = tf.keras.layers.LSTM(512, return_state=True, name='lstm')(x)\n",
        "    model = tf.keras.models.Model(inputs=model_input, outputs=[hidden_state, cell_state], name='Encoder')\n",
        "    return model"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDwIW6l-DVhk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61a03929-c88f-4f0e-8375-22a23d56268b"
      },
      "source": [
        "encoder = get_encoder((None, 128))\n",
        "\n",
        "english_batch, german_batch = next(iter(training_batched_dataset.take(1)))\n",
        "\n",
        "hidden_state, cell_state = encoder(english_batch)\n",
        "\n",
        "print(f'hidden_state shape: {hidden_state.shape}')\n",
        "print(f'cell_state shape: {cell_state.shape}')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hidden_state shape: (16, 512)\n",
            "cell_state shape: (16, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEk9ikVh1ibL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba45d65b-fe23-4380-f512-b8fcfe360afe"
      },
      "source": [
        "# model summary for the encoder network.\n",
        "encoder.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Encoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, None, 128)]       0         \n",
            "_________________________________________________________________\n",
            "end_token_embedding_layer (E (None, None, 128)         128       \n",
            "_________________________________________________________________\n",
            "masking_layer (Masking)      (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  [(None, 512), (None, 512) 1312768   \n",
            "=================================================================\n",
            "Total params: 1,312,896\n",
            "Trainable params: 1,312,896\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvkzpCeZ1ibR"
      },
      "source": [
        "##Decoder network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l50qhnXD1ibT"
      },
      "source": [
        "class Decoder(tf.keras.models.Model):\n",
        "\n",
        "    def __init__(self, german_tokenizer_max_index,**kwargs):\n",
        "        super(Decoder, self).__init__(**kwargs)\n",
        "        self.embedding = tf.keras.layers.Embedding(german_tokenizer_max_index + 1, 128, mask_zero=True, name='embedding')\n",
        "        self.lstm = tf.keras.layers.LSTM(512, return_state=True, return_sequences=True, name='lstm')\n",
        "        self.dense = tf.keras.layers.Dense(german_tokenizer_max_index + 1, name='dense')\n",
        "        \n",
        "    def call(self, inputs, hidden_state, cell_state):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.lstm(x, initial_state=[hidden_state, cell_state])\n",
        "        return self.dense(x[0]), x[1], x[2]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1QbVGo80BQ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3849f886-f092-4f95-dff5-8af8b6c1cd87"
      },
      "source": [
        "english_batch, german_batch = next(iter(training_batched_dataset.take(1)))\n",
        "\n",
        "encoder = get_encoder((None, 128))\n",
        "hidden_state, cell_state = encoder(english_batch)\n",
        "\n",
        "decoder = Decoder(german_tokenizer_max_index, name='Decoder')\n",
        "output, hidden_state, cell_state = decoder(german_batch, hidden_state, cell_state)\n",
        "\n",
        "print(f'output shape: {output.shape}')\n",
        "print(f'hidden_state shape: {hidden_state.shape}')\n",
        "print(f'cell_state shape: {cell_state.shape}')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output shape: (16, 14, 5744)\n",
            "hidden_state shape: (16, 512)\n",
            "cell_state shape: (16, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFYU5-XezuKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75eab313-114f-498f-bce2-f23594ed9083"
      },
      "source": [
        "# model summary for the decoder network\n",
        "decoder.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  735232    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  multiple                  1312768   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  2946672   \n",
            "=================================================================\n",
            "Total params: 4,994,672\n",
            "Trainable params: 4,994,672\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pST9XGJ81ibo"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hJHbWqs1ibr"
      },
      "source": [
        "# function that takes a Tensor batch of German data (as extracted from the \n",
        "# training Dataset), and returns a tuple containing German inputs and outputs for the decoder model\n",
        "\n",
        "def get_german_data(batch_data):\n",
        "    return batch_data[:,:-1], batch_data[:,1:]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVDPbZwUSKYN"
      },
      "source": [
        "# computes the forward and backward pass for your translation model.\n",
        "\n",
        "@tf.function\n",
        "def propogate(english_input, german_input, german_output, encoder, decoder, loss_fn):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      encoder_hidden_state, encoder_cell_state = encoder(english_input)\n",
        "      decoder_return_sequence, _, _ = decoder(german_input, encoder_hidden_state, encoder_cell_state)      \n",
        "      loss = loss_fn(german_output, decoder_return_sequence)\n",
        "    return loss, tape.gradient(loss, [encoder.trainable_variables, decoder.trainable_variables])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVWOYJ3l1ib1"
      },
      "source": [
        "# custom training loop\n",
        "\n",
        "def train(encoder, decoder, num_epochs, training_batched_dataset, validation_batched_dataset, patience):\n",
        "\n",
        "    training_epochs_losses = []\n",
        "    validation_epochs_losses = []\n",
        "\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    patience_count = 0\n",
        "    for i in range(num_epochs):\n",
        "        \n",
        "        training_epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "        validation_epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "\n",
        "        for english, german in training_batched_dataset.as_numpy_iterator():\n",
        "\n",
        "            german_input, german_output = get_german_data(german)\n",
        "            loss, [encoder_gradient, decoder_gradient] = propogate(english, german_input, german_output, encoder, decoder, loss_fn)\n",
        "            training_epoch_loss_avg(loss)\n",
        "            optimizer.apply_gradients(zip(encoder_gradient, encoder.trainable_variables))\n",
        "            optimizer.apply_gradients(zip(decoder_gradient, decoder.trainable_variables))\n",
        "\n",
        "        for english, german in validation_batched_dataset:\n",
        "\n",
        "            german_input, german_output = get_german_data(german)\n",
        "            encoder_hidden_state, encoder_cell_state = encoder(english)\n",
        "            decoder_return_sequence, _, _ = decoder(german_input, encoder_hidden_state, encoder_cell_state)\n",
        "            validation_epoch_loss_avg(loss_fn(german_output, decoder_return_sequence))\n",
        "\n",
        "\n",
        "        validation_epoch_loss = validation_epoch_loss_avg.result().numpy()\n",
        "\n",
        "        if not validation_epochs_losses or validation_epoch_loss < min(validation_epochs_losses):\n",
        "            print('saving best weights')\n",
        "            encoder.save_weights('./encoder/checkpoint')\n",
        "            decoder.save_weights('./decoder/checkpoint')\n",
        "\n",
        "        if validation_epochs_losses and validation_epoch_loss > min(validation_epochs_losses):\n",
        "            patience_count += 1\n",
        "        else:\n",
        "            patience_count = 0\n",
        "\n",
        "        if patience_count > patience:\n",
        "            break\n",
        "\n",
        "        validation_epochs_losses.append(validation_epoch_loss)\n",
        "\n",
        "        training_epoch_loss = training_epoch_loss_avg.result().numpy()\n",
        "        training_epochs_losses.append(training_epoch_loss)\n",
        "        \n",
        "\n",
        "        print(f'epoch: {i+1}/{num_epochs}, loss: {str(round(training_epoch_loss, 2))}, val_loss: {str(round(validation_epoch_loss, 2))}')\n",
        "        if patience_count > patience:\n",
        "            break\n",
        "    \n",
        "    return training_epochs_losses, validation_epochs_losses"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKVbRqxWZld-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3e12f21-1f6e-4d60-8768-acf5b154b934"
      },
      "source": [
        "encoder = get_encoder(input_shape=(None, 128))\n",
        "decoder = Decoder(german_tokenizer_max_index)\n",
        "num_epochs = 15\n",
        "patience = 1\n",
        "\n",
        "training_batched_dataset = get_batched_dataset(training_dataset, training=True)\n",
        "validation_batched_dataset = get_batched_dataset(validation_dataset)\n",
        "\n",
        "training_epochs_losses, validation_epoch_losses = train(encoder, decoder, num_epochs, training_batched_dataset, validation_batched_dataset, patience)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saving best weights\n",
            "epoch: 1/15, loss: 5.32, val_loss: 4.59\n",
            "saving best weights\n",
            "epoch: 2/15, loss: 4.02, val_loss: 3.68\n",
            "saving best weights\n",
            "epoch: 3/15, loss: 3.09, val_loss: 2.93\n",
            "saving best weights\n",
            "epoch: 4/15, loss: 2.28, val_loss: 2.27\n",
            "saving best weights\n",
            "epoch: 5/15, loss: 1.57, val_loss: 1.73\n",
            "saving best weights\n",
            "epoch: 6/15, loss: 1.02, val_loss: 1.37\n",
            "saving best weights\n",
            "epoch: 7/15, loss: 0.66, val_loss: 1.17\n",
            "saving best weights\n",
            "epoch: 8/15, loss: 0.45, val_loss: 1.07\n",
            "saving best weights\n",
            "epoch: 9/15, loss: 0.33, val_loss: 1.02\n",
            "saving best weights\n",
            "epoch: 10/15, loss: 0.25, val_loss: 1.01\n",
            "saving best weights\n",
            "epoch: 11/15, loss: 0.2, val_loss: 1.0\n",
            "epoch: 12/15, loss: 0.16, val_loss: 1.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgyayfBziKPq"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpacst2F1ib7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "de2d49dc-734e-4117-c25e-df2e9d719e7a"
      },
      "source": [
        "# loss vs epoch for both training and validation sets.\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(training_epochs_losses)\n",
        "plt.plot(validation_epoch_losses)\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.title('Loss v/s epochs');"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gYTa3xwhOvX"
      },
      "source": [
        "##Load best weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msihYhzlgqUG",
        "outputId": "0a69c1c2-fc76-4210-b1c7-905bccd7bbc3"
      },
      "source": [
        "decoder = Decoder(german_tokenizer_max_index, name='Decoder')\n",
        "decoder.load_weights('./decoder/checkpoint')\n",
        "\n",
        "encoder = get_encoder((None, 128))\n",
        "encoder.load_weights('./encoder/checkpoint')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd40d4cdfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2TQ8T8zikdU"
      },
      "source": [
        "##Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVOa8JLB0GWK"
      },
      "source": [
        "english_sentences, german_sentences = get_sentence(data_examples)\n",
        "\n",
        "split = train_test_split(english_sentences, german_sentences, test_size=0.2, random_state=5)\n",
        "_ , english_sentences_test = split[:2]\n",
        "_ , german_sentences_test = split[2:]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyEPp0AErix0"
      },
      "source": [
        "# Preprocess and embed the English sentence according to the model requirements.\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((english_sentences_test, german_sentences_test))\n",
        "test_dataset = split_english_sentences(test_dataset)\n",
        "test_dataset = embed_english_words(test_dataset, embedding_layer)\n",
        "test_dataset = filter_english_seq(test_dataset)\n",
        "# test_dataset = pad_english_seq(test_dataset)\n",
        "test_dataset = get_batched_dataset(test_dataset, batch_size=1)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu3AiJk3k-4p"
      },
      "source": [
        "# Pass the embedded sentence through the encoder to get the encoder hidden and cell states.\n",
        "# Decode the output token sequence into German text and print the English text and the model's German translation.\n",
        "start_token = german_tokenizer.word_index['<start>']\n",
        "end_token = german_tokenizer.word_index['<end>']\n",
        "\n",
        "total_bleu_score = 0\n",
        "\n",
        "for i, (english_data, _) in enumerate(test_dataset):\n",
        "    hidden_state, cell_state = encoder(english_data, training=False)\n",
        "    decoder_output = []\n",
        "    token = start_token\n",
        "    for j in range(14):\n",
        "      token = tf.constant([[token]], dtype=tf.float32)\n",
        "      logit, hidden_state, cell_state = decoder(token, hidden_state, cell_state, training=False)\n",
        "      token = tf.argmax(logit[0][0]).numpy()\n",
        "      if token == end_token:\n",
        "        break\n",
        "      decoder_output.append(token)\n",
        "\n",
        "    reference = german_sentences_test[i]\n",
        "    reference = \" \".join(re.split('\\W+', reference))\n",
        "    reference = reference.strip()\n",
        "    reference = reference.split()\n",
        "\n",
        "    hypothesis = \" \".join([german_tokenizer.index_word[token] for token in decoder_output])\n",
        "    hypothesis = \" \".join(re.split('\\W+', hypothesis))\n",
        "    hypothesis = hypothesis.strip()\n",
        "    hypothesis = hypothesis.split()\n",
        "    \n",
        "    smoothie = SmoothingFunction().method1\n",
        "    bleu_score = sentence_bleu([reference], hypothesis, smoothing_function=smoothie)\n",
        "    total_bleu_score += bleu_score\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8XQLdERwWu6",
        "outputId": "f01c7b7f-1d67-486a-81bc-42a55ca48147"
      },
      "source": [
        "print(f'bleu score : {total_bleu_score / len(english_sentences_test)}')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bleu score : 0.19515308627141179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM2gvBM11ib-"
      },
      "source": [
        "##Translate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGnQtE7L1icA"
      },
      "source": [
        "# five randomly sampled English sentences\n",
        "idx = tf.random.uniform((5,), minval=0, maxval=len(english_sentences_test), dtype=tf.int32).numpy()\n",
        "\n",
        "five_english_sentences , five_german_sentences = [], []\n",
        "\n",
        "for id in idx:\n",
        "    five_english_sentences.append(english_sentences_test[id])\n",
        "    five_german_sentences.append(german_sentences_test[id])"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohleJRcJ1icD"
      },
      "source": [
        "# Preprocess and embed the English sentence according to the model requirements.\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((five_english_sentences, five_german_sentences))\n",
        "test_dataset = split_english_sentences(test_dataset)\n",
        "test_dataset = embed_english_words(test_dataset, embedding_layer)\n",
        "test_dataset = filter_english_seq(test_dataset)\n",
        "# test_dataset = pad_english_seq(test_dataset)\n",
        "test_dataset = get_batched_dataset(test_dataset, batch_size=1)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unk60cEy1icI"
      },
      "source": [
        "# Pass the embedded sentence through the encoder to get the encoder hidden and cell states.\n",
        "states = []\n",
        "for english_data, _ in test_dataset:\n",
        "    states.append(encoder(english_data))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo-yyzs6xCWJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db82fefd-9c47-4d53-f5e1-294172430caf"
      },
      "source": [
        "# Decode the output token sequence into German text and print the English text and the model's German translation.\n",
        "start_token = german_tokenizer.word_index['<start>']\n",
        "end_token = german_tokenizer.word_index['<end>']\n",
        "\n",
        "for i in range(5):\n",
        "    if i != 0:\n",
        "        print('\\n==================================================================================\\n')\n",
        "    decoder_output = []\n",
        "    hidden_state, cell_state = states[i][0], states[i][1]\n",
        "    token = start_token\n",
        "    for j in range(14):\n",
        "      token = tf.constant([[token]], dtype=tf.float32)\n",
        "      logit, hidden_state, cell_state = decoder(token, hidden_state, cell_state, training=False)\n",
        "      token = tf.argmax(logit[0][0]).numpy()\n",
        "      if token == end_token:\n",
        "        break\n",
        "      decoder_output.append(token)\n",
        "\n",
        "    print(f'english sentence             : {five_english_sentences[i]}')\n",
        "    print(f'labeled german translation   : {five_german_sentences[i]}')\n",
        "    print(f'predicted german translation : {\" \".join([german_tokenizer.index_word[token] for token in decoder_output])}')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english sentence             : i'm a journalist .\n",
            "labeled german translation   : ich bin journalist .\n",
            "predicted german translation : ich bin lehrer .\n",
            "\n",
            "==================================================================================\n",
            "\n",
            "english sentence             : tom was confused .\n",
            "labeled german translation   : tom war verwirrt .\n",
            "predicted german translation : tom war verbluefft .\n",
            "\n",
            "==================================================================================\n",
            "\n",
            "english sentence             : we knew this .\n",
            "labeled german translation   : das wussten wir .\n",
            "predicted german translation : wir kannten das .\n",
            "\n",
            "==================================================================================\n",
            "\n",
            "english sentence             : do you do yoga ?\n",
            "labeled german translation   : macht ihr yoga ?\n",
            "predicted german translation : betreiben sie yoga ?\n",
            "\n",
            "==================================================================================\n",
            "\n",
            "english sentence             : i stood up slowly .\n",
            "labeled german translation   : ich stand langsam auf .\n",
            "predicted german translation : ich bin ueber nacht .\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}